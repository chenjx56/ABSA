# Bridge-Based Active Domain Adaptation for Aspect Term Extraction

面向属性抽取的基于桥的主动域自适应



## Abstract

作为一个细粒度的任务，标注数据非常昂贵。最近有尝试使用领域适应技术来缓解这个问题，即转移跨领域的公共知识。

现存的解决方案是连接属性词与中心词（消极领域适应，依赖于属性词与中心的连接），需要人工标记中心和昂贵的计算资源来建立关联。

我们提出了积极领域适应的办法，我们的目标是通过积极补充可转移知识来转移方面术语。

为此，我们通过将句法角色识别为中心（不是链接到中心）来构建句法桥梁。我们还通过检索可转移的语义原型来构建语义桥梁。



## Conclusion

本文提出了一种新的主动域自适应方法来提取属性词。

不同于以往的被动域适应研究（连接属性词与中心），我们通过构建词的句法和语义桥梁来增强词的可移动性。然后我们设计了一个轻量级的端到端标记器用于基于桥的序列标记。

我们的方法以相当低的计算成本实现了一种新的最先进的性能。



## Introduction

为了解决数据缺乏的问题，提出了无监督域自适应方法，将知识从标记源领域转移到未标记目标域。

问题：属性词大都与领域相关，无法直接转移。

![image-20211101093602345](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20211101093602345.png)

哪怕是领域非常相关的L与D，相同属性词的比例也没有超过40%。因此，许多属性词必须在适当参考的指导下进行转换。



过去通过链接属性词与特定的中心，有两种方案：

1. 将意见词作为中心：早期要使用相同的意见种子和预先定义的规则，但很难收集完全意见种子，也很难定义高质量的规则，所以性能都很差。还有几项研究手动注释评论中的所有意见词，并设计神经模型，通过多任务学习捕获属性-意见关系。尽管性能提升，但是增加了额外的标记成本。
2. 上下文作为中心：因为像BERT这样的预训练语言模型是根据上下文表示单词，所以可以利用预训练语言模型结合相同的上下文词来转移属性词。然而，并非所有上下文词语都有资格作为中心。此外，像BERT这种预训练语言模型主要基于共现上下文中的语义相似度建立词关联。如下图中的例子所示，这些方法很难根据源领域中的标记词pizza来识别目标领域中的keyboard。

![image-20211101100502635](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20211101100502635.png)

上图展示了如何基于源词pizza识别没见过的目标词keyboard：

1. **句法桥**：识别跨域词的可转移的句法角色。我们将某个词所涉及的句法角色(包括词性标签和依存关系)作为其句法桥梁。【是将句法角色作为中心特征，不需要任何的手工标记中心词】
2. **语义桥**：通过检索可转移的原型更进一步。使用句法增强的相似度评价指标检索可转移的原型。构建语义桥梁直接连接跨领域的属性词，只需要未标记的源数据和目标数据。

基于这两个桥，我们开发了端到端的标记器，将评论与这些可转移的桥融合到一起。



## Related Work

早期用预定义的规则、手工设计的特征进行ATE。随着深度学习的发展，有监督的序列标记器成为主流。最近新兴了许多研究，让ATE与其他任务进行交互，比如：属性级情感分类。这些方法非常依赖领域特定的训练数据，因此开发无监督的领域适应方法更实用。

许多领域自适应方法已经被提出来解决像文本分类这样的粗粒度任务。粗粒度任务的基本思想是转移中心词，但这并不适合ATE，因为大多数属性词都是特定领域的非中心词。对这个问题已经做了一些尝试，分为两种，上一节已经介绍。



## Model

### Problem Statement

给的一个评论句$x = \{x_1,...,x_n\}$，预测 $y=\{y_1,..,y_n\}，y_i∈\{B,I,O\}$

给定一系列来自源领域的标记好的数据$D^S=\{(x^S_j,y^S_j)\}^{N_S}_{j=1}$，和一系列来自目标领域的未标记的数据$D^U=\{(x^U_j)\}^{N_U}_{j=1}$，我们的目标是从没见过的目标集$D^T=\{(x^T_j)\}^{N_T}_{j=1}$中预测标记$y^T$。



### Bridge Construction

首先，映射$X\rightarrow E$。对于跨域ATE，我们为评论构建桥梁，以帮助直接跨两个域转移属性词。

**Syntactic Bridge**

不同领域中的句法结构其实都差不多。我们可以根据源词和目标词的句法角色建立它们之间的关联。如下图所示，“The keyboard is in reasonable size.  ”与其有相似的分析结果，因此句法角色可以作为一种补充证据来识别跨领域的属性词。

![image-20211101104957284](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20211101104957284.png)

我们的目标是，通过将它们自己识别为中心，充分利用句法角色，而不是将它们视为与枢轴的链接。

为了实现这一目标，我们提出了一种新的数据结构，通过将词性标记和依赖关系的信息注入到相关单词中来编码它们。

如下图，使用one-hot向量代表POS标记，使用multi-hot向量代表依存关系，维度取决于各自的类别数量。然后用可学习的权重矩阵将它们映射成同样的维度并且进行合并，最终得到句法桥$b_{syn}$，它和embedding的维度是一致的。

![image-20211101110659404](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20211101110659404.png)

![image-20211101184003950](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20211101184003950.png)

**Semantic Bridge**

语义桥以上面的语法角色为基础，但进一步检索可转移的原型。例如，为了将知识从$D^S$中的pizza转移到$D^T$中的keyboard，我们要为pizza引入$D^U$中的$\{disk、OS、mouse\}$等补充的目标词，直接提高其与keyboard的语义相关性。我们称这些补充词为原型，并将检索它们以构建语义桥梁。

我们设计了一个语法增强的相似度度量来检索可转移的语义原型。

![image-20211101185419448](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20211101185419448.png)

首先过滤$D^U$中单词，只保留出现超过τ（最终设置为5）次的，作为候选原型，并据此从$D^U$中构建一个原型库$\widetilde{V}$。

对于一个来自$V^S（D^S的词表）$中的查询词$v$，我们要从$\widetilde{V}$中找到原型词$\widetilde{v}$。它们在目标领域都扮演着相同的句法角色。具体步骤：（1）通过合并$D^S$中所有$v$出现过的评论句的词性和依存嵌入向量得到$v$的全局用法的摘要。

![image-20211101183912950](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20211101183912950.png)

（2）利用相同的方法，求得关于$\widetilde{v}$的$\widetilde{b}^g_{pos}$和$\widetilde{b}^g_{dep}$

（3）求余弦相似度，最后得到相似矩阵

![image-20211101184330390](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20211101184330390.png)

选择得分最高的K个词，和他们的分数，合并它们的原型到v的语法桥$b_{sem}$：

![image-20211101184612980](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20211101184612980.png)

我们也为$D^U$和$D^T$检索可转移的原型，通过这个方法，带有相同原型的源词与目标词可以直接关联起来。

$D^T$用于测试，是不可见的，所以无法获得全局的$b^g_{pos}/b^g_{dep}$，因此对应$D^T$中的词$w$，如果它出现在$D^U$则直接通过$M^U$获得$b_{sem}$，否则，暂时使用当前测试样本中w的局部的$b_{pos}/b_{dep}$来代替全局的。

**Bridge-based Sequence Tagging**

![image-20211101185345222](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20211101185345222.png)



Bridege Fuser

我们建造的桥有两个特性：

1. 桥是领域不变的，应该保留。
2. 桥可以帮助从$e_i$中提取领域不变的信息。

因此，我们提出利用词$x_i$的可转移桥$b_{syn,i}$和$b_{sem,i}$来增强词$x_i$的嵌入$e_i$。具体来说，我们用一个门控操作来熔合桥梁。

![image-20211101185804295](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20211101185804295.png)

![image-20211101185819831](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20211101185819831.png)

$e_{sem,i}$也可以这样计算得到。

$e_i$，$b_{syn,i}$和$b_{sem,i}$ 分别作为BaseTagger，SynBridge和SemBridge的输入



Feature Extractor

利用四个卷积层抓取特征

![image-20211101190148548](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20211101190148548.png)



Token Classifier

用卷积层最后的输出作为输入，进行token的分类

![image-20211101190300022](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20211101190300022.png)



Domain Classifier

除了BIO标记外，我们通过领域对抗训练进一步增强了基于桥的特征的领域不变性。首先合并卷积层的各个输出作为一个卷积表示。

![image-20211101190430423](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20211101190430423.png)

然后使用一个梯度逆转层（GRL），训练一个领域分类器来区分领域。

<img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20211101190617693.png" alt="image-20211101190617693" style="zoom:80%;" />



Training Procedure

使用的是交叉熵损失函数。

![image-20211101190652943](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20211101190652943.png)

![image-20211101190711359](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20211101190711359.png)

$y_d=0\ for\ D^S,  y_d=1\ for\ D^U$

![image-20211101190817158](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20211101190817158.png)



## Experiment

### Datasets

![image-20211101190912142](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20211101190912142.png)

![image-20211101190926618](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20211101190926618.png)

BaseTagger已经比所有baseline好，归因于利用了CNN的特征抽取和领域对抗训练。

CNN关注N元特征而非单个词的特征，减少了非中心属性词的副作用。领域对抗训练应用于句子级特征，这样他们就不会被标记为0和1的常见N元所误导。

SemBridge比SynBridge好一点，原因有两点：

1. 语义桥来源于具有先验嵌入知识并包含句法信息的原型词，而句法桥则是从头开始训练的。
2. 检索到的top-K词使得SemBridge中的补充信息比SynBridge中的更加多样化和丰富。



## Analysis

### OTE Task？

表现依旧很好。



### Ablation Study

![image-20211101191756400](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20211101191756400.png)



### Parameter Study  

![image-20211101191851136](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20211101191851136.png)

上图，考虑三种参数。

![image-20211101192018270](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20211101192018270.png)

上图考虑未标记数据的比例，与语法分析噪声的比例。







